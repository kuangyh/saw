# Saw

## Background: filling the gaps

Topic: Log processing, or "ETL" as buzzword speakers called.

A few driven usecases from a user-facing Web app.

- User session:
  - Many analysis tasks use session data as input. Although we have huge amount of sessions, data of a single session is small enough to be represented as in memory array, be analyzed using conventional programming languages.
  - Logs from clients and severs needed to be aggregated by session-id, merged, converted into consistent form, and be available for analysis ASAP, daily job is not good.
  - Logs are often noisy, need to filter / clean up / normalize, this process is often complicated and buggy.
- Dimensional metrics
  - One common use case for user session data is to further aggregate them into many dimensional metrics.
  - For each session, we get a set of fine-grained dimensions, and a set of events (or event flow / compund event), they will be stored into PostgreSQL / Dremel / Hive as a conventional flat structure (A row with pre-defined columns).
  - Then we can query them in an ad-hoc way, it helps answer questions like "what will China user likely do after clicking this button", or "is this experiment has signficiantly higher CTR just on a specific device or on all platforms in general"
  - Talking about metrics, it's not just SUM / MEAN, confidence interval, quantile / percentile are extremely important.


ETL jobs is NOT data analysis, they provide clean, well-structured datasource for data analysis. These jobs are often complicated because of dirty nature of input, ever changing bussiness logic, and the cost of maintaing 24/7 data pipelines with complex dependency:

- Dirty input: data input are often heterogeneous and noisy, converting input and cleaning up is complicated, buggy, and often tightly bounded to bussiness-logic.
- 24/7 pipelines: ingestion pipelines are designed to run 24/7 or hourly / daily, either way, they're not ad-hoc, despite being frequently updated to address bussiness needs, the computation structure is largely well-defined and stable.
- Streaming + small batch: just like user session case, it needs both streaming support and small hourly batches.
- Data dependency: there're many data analysis pipelines with complex data dependency --- some process hourly output from another, some use daily and some use realtime result, some use compunded data sources.

Serveral tools existed to address some of the problems, but there're gaps.

Log ingesting tools like Fluented or Heka provides configurable framework that unifies logs from multiple formats (because they're generated by different softwares), and store to different storage backend in a manageable way, but:

- Converting input formats is actually simple job (after all, you should use structual logging, like proto logs), but cleaning up / normalize data is not, it requires real programming and often tightly bussiness logic coupled.
- Mass majority of their features focus on 1:1 mapping of data, but lack of small batch support.

There're too many shinely new computation frameworks out there, Spark, Flume... they solve data analysis problems well, but they doesn't solve the real complexity of ETL job mentioned ahove.

Most notably, new frameworks like Spark or Datflow/Flume invested heavily on automate or optimize computation with complex structures, Flume for example, provides planers that auto structure program into multiple MRs, but most ETL job's computation structure are quite simple --- one pass MR for maybe 95% of cases, for the remaining 5%, hand-tuned pipelines would often provide better ROI because jobs are stable not ad-hoc, hand-optimized job save morecomputation cost in the long run than programmer time saving from job planner / compiler.

In order to support complex parallel computing, these frameworks often introduce functional programing model, despite exccelent for many computation / data analysis / ML problems, I argue it's not a good fit for ETL jobs. (TODO: more explanation with non-confidential examples)

There're workflow management tools based on DAG like [Airflow](http://nerds.airbnb.com/airflow/) that solves data dependency and work scheduling problem.

In conclusion, we need:

- An imperative programming model that helps handle dirty data and complex biz logic, but at the same time be able to scale.
- Make use of current infrastructures: MapReduce, Kafka / NSQ, Airflow to name a few, don't re-invent another computation platform.

## Programming model

Saw, the programming model we described here is heavily influented by [Sawzall](http://static.googleusercontent.com/media/research.google.com/en//archive/sawzall-sciprog.pdf) and [Dataflow Model](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43864.pdf). Tt takes Table abstract from Sawzall, and more generalized computation model from Dataflow, readers with experience of these either frameworks may find Saw quite familiar.

### Datum

Datum is the basic data unit passing around Saw, it's a KV pair, where K must be a byte sequence and V can be anything.

### Saw

Saw is the basic computation unit, it's essentially a state machine with two basic interfaces:

- `Emit(Datum)` receives a new data point, changes internal state, send data to other saws.
- `Result() -> Any` ends processing, provide final result.

Saws can be dynamically created and removed (after calling Result()), it's common to have many instances of same kind of saws distributed across machines to process data in parallel.

### Table

Table manages a map of datum key to the saw dedicated to this key. when a datum comes to a table, it will be routed to the saw corresponded to datum's key. For example:

- Sum is a kind of Saw, that sums all value of datum inputted.
- Table of Sum, is a table, that when a datum come, creates a Sum saw if `dataum.key` not in table yet, then routes the datum to the corresponding saw.
- The classical word count problem can be solved by emiting `<word, 1>` pairs to Table of Sum for each word in sentence.

Table can dump computation result of its items to persistent store, as `<key, Result()>` pairs, note that this result is still in datum formed, can be used as input for other pipelines, it's often called "re-saw".

Table has Saw interface, but it can logically run across instances. When a we emits data to a table by simply call `someTable.Emit(datum)`, we don't need to care if it does the computation and manages state in local memory, or sends it to another node.

There're also special tables that doesn't manages item saws, but simply stores each datum received (that forms a multi-dict, best fit for re-saw), or send it to message brokers.

### Aggregators

Saws like Sum are aggregators, aggregators are stateful saws that accuminates input on the fly with low time and space complexity. Combined table and aggregators like above, we can easily do large scale aggregation work.

A lot of aggregators can be "merged", that means to further aggregate some aggregated result, it serves at least two kind of needs:

- Local partial aggregation: In distributed batch aggregation (assuming MR), data can be partially aggregated at local, before sending to reduce nodes, that saves a lot of bandwidth.
- Dimension merge: in dimentional metrics case, metrics are aggregated for each fine-grained group, but we need merged result for roll-up views.

Some aggregators are simple, like Sum, but some like quantile are significantly more complex, we'll implement a set of common aggregators.

### Driver

Driver dynamically creates saws needed and feeds data source into them, it manages data parallelism and distribution.

There're multiple kinds of drivers, a distributed batch processing driver may start many processes at remote machines, each one process one shard, then at each process a local driver will create tables / saws instances and parallelly feeds data to them (implemented by wrapping MR framework). A message consumer driver can work as Kafka / NSQ's consumer, route messages to saws in one-request-one-goroutine maner.

## Implementation

### Early High-level decisions

- Go
  - It's not data analysis --- or I'd better use Python
  - Static type, code analysis tool, enable debugging and auditing at scale.
  - Acceptable performance at many-core architecture.
  - Easy deploy
- Optimize of single instance first
  - Single machine is powerful enough to cover 80% of needs today, Go uses multi-cores well.
  - They're very cheap on cloud --- 32 cores, 120G RAM preemptible instance costs < $0.5 / hr on [GCE](https://cloud.google.com/compute/pricing), and you can simply disable it when it's not actively used.
  - Familiar, flexible programming model from start, worry about horizontal scalability later.
- Optimize for Ops
  - Monitoring: critical for long running jobs.
  - Ease deployment --- single binary, different saw / driver configuration on different nodes, update and restart the whole thing in one shot.
  - Pipeline / data dependency management and scheduling.
- Cloud infrastructures
  - Cheap network storage like S3 or GCS can be as fast as local SSD. See also [Latency Numbers Every programmer Should Know](http://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html)
  - Off-the-shelf haddop + spark cluster.
  - Computer instances can be start / stop within minutes, and even docker and kubernetes, oh my...
  - Saw won't re-invent the wheel for all these stuff already there.

### Local instance

### Tables

### Distributed computing

### Persistency / Storage

### Fault tolerence

## Example: session + dashboard
